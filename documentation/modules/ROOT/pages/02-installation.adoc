[2. Laboratorio]
== Preparar el entorno del laboratorio

Para comenzar, establecemos un entorno virtual de Python que nos permitirá interactuar con InstructLab. Luego, instalaremos Instructlab.

[source,bash]
----
cd ~/instructlab
source venv/bin/activate
pip install git+https://github.com/instructlab/instructlab.git@v0.17.1
----

¡Perfecto! Ya podemos usar Instructlab con el comando `ilab`. Ahora vamos a inicializar el entorno para empezar a trabajar con los modelos:

[source,bash]
----
ilab config init
-> Presiona enter cuando el comando pida argumentos.
----

Durante la fase de inicialización ocurren varias cosas:

* Se localiza una taxonomía por defecto en el sistema de archivos local.
* Se crea un archivo de configuración (config.yaml) en el directorio actual.

El archivo *config.yaml* contiene los valores predeterminados que utilizaremos en este taller. Este archivo nos permite ajustar los parámetros a nuestro gusto.

== Descargar y servir el modelo

Con el entorno configurado, ahora podemos descargar un modelo cuantificado (comprimido y optimizado) al directorio local para ser utilizado como un servidor de modelos para las solicitudes de la API, o para ayudar a entrenar a un nuevo modelo como haremos en este taller.

[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

El comando `ilab model download` descarga el modelo Granite 7b Lab de los repositorios de HuggingFace. Ahora que hemos descargado el modelo, podemos servir y chatear con él. Servir el modelo significa que vamos a ejecutar un servidor que permitirá a otros programas interactuar con los datos de forma similar a hacer una llamada a la API.

[source,bash]
----
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

El comando `serve` puede tomar un argumento opcional --model-path. En este caso, queremos servir el modelo Granite. Si no se proporciona ninguna ruta de modelo, se utilizará el valor predeterminado del archivo *config.yaml*.

WOOHOO! Con el modelo servido, ya estamos listos para probar el LLM. Para ello, vamos a chatear con el modelo.

== Chatear con el modelo

Vamos a dejar el modelo sirviendose en el terminal donde hemos trabajo y abriremos una nueva pestaña del terminal. Volveremos a activar el entorno virtual de Python.

[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

Ya dentro del entorno, podemos iniciar una sesión de chat con el comando ilab chat:

[source,bash]
----
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help)
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

¡Genial! Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir una pregunta, por ejemplo: ¿Qué es Openshift en 20 palabras o menos?

[source,bash]
----
¿Qué es Openshift en 20 palabras o menos?
----